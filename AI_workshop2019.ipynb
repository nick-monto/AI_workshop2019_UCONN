{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_workshop2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ZPMUExjTfZbA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import git\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import tempfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ijEy_xsLlTtR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this will download the necessary data we will be working with\n",
        "if not os.path.exists('./AI_workshop2019/swda_master'):\n",
        "    print('Downloading the most recent snapshot of the swda git repo...')\n",
        "    # download the swda repository from github\n",
        "    git.Repo.clone_from('https://github.com/cgpotts/swda', './AI_workshop2019/swda_master', branch='master', depth=1)\n",
        "    # Create temporary dir\n",
        "    t = tempfile.mkdtemp()\n",
        "\n",
        "    # unzip the zipped corpus located in the swda_master folder\n",
        "    with zipfile.ZipFile('./AI_workshop2019/swda_master/swda.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall(t)\n",
        "\n",
        "    # move the corpus folder to the current master folder then remove the temp dir\n",
        "    shutil.move(os.path.join(t, 'swda'), './AI_workshop2019/swda_master')\n",
        "    shutil.rmtree(t)\n",
        "\n",
        "    print('Successfully downloaded the swda repository!')\n",
        "\n",
        "## creates an html file that contains the structure of the directory\n",
        "#os.system('tree -H ./AI_workshop2019/swda_master > ./AI_workshop2019/directory_tree.html')\n",
        "## prints the files/folders in the immediate directory and their sizes\n",
        "#print('Here is a glimpse of the directory structure.')\n",
        "#os.system('du -sh ./AI_workshop2019/swda_master/*')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPidymfynLFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will import the packages required to look at the data."
      ]
    },
    {
      "metadata": {
        "id": "jCxeul1kmeCs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# QOL\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# stats\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# function from swda dataset\n",
        "import sys\n",
        "sys.path.insert(0, './AI_workshop2019/swda_master/')\n",
        "\n",
        "from swda import CorpusReader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R8jWRCJLnmGC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def treebank_pos_dist():\n",
        "    \"\"\"Build a POS relative frequency distribution for the NLTK subset of the WSJ Treebank.\"\"\"\n",
        "    d = defaultdict(int)\n",
        "    for fileid in treebank.fileids():\n",
        "        for word in treebank.tagged_words(fileid):\n",
        "            d[word[1]] += 1\n",
        "    dist = {}\n",
        "    total = float(sum(d.values()))\n",
        "    for key, val in d.items():\n",
        "         dist[key] = d[key] / total\n",
        "    return dist\n",
        "\n",
        "def tag_dist():\n",
        "    \"\"\"Gather and print relative frequency distribution of the tags.\"\"\"\n",
        "    d = defaultdict(int)\n",
        "    corpus = CorpusReader('swda')\n",
        "    # Loop, counting tags:\n",
        "    # the inclusion of tqdm() wrapper provides a timer for the for loop\n",
        "    # if the length of the object is known, a progress bar is displayed\n",
        "    # if unknown, then you just get a standard timer\n",
        "    for utt in tqdm(corpus.iter_utterances(display_progress=False)):\n",
        "        for x in np.asarray(utt.pos_lemmas(wn_lemmatize=False)):\n",
        "            d[x[1]] += 1\n",
        "    dist_swda = {}\n",
        "    total = float(sum(d.values()))\n",
        "    for key, val in d.items():\n",
        "        dist_swda[key] = d[key] / total\n",
        "    return dist_swda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JYK-H4Ppntc3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "parent_dir = os.getcwd()\n",
        "os.chdir('./AI_workshop2019/swda_master/')\n",
        "\n",
        "# check out the dictionary generated for the swda corpus\n",
        "swda_POSdist = tag_dist()\n",
        "swda_POSdist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OjUr3IyIKht0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# and for the WSJ subset\n",
        "nltk.download('treebank')\n",
        "WSJ_POSdist = treebank_pos_dist()\n",
        "WSJ_POSdist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJKv-PEhKmcs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get total counts of the POS tags for each dataset\n",
        "count = 0\n",
        "for x in WSJ_POSdist.keys():\n",
        "    count += 1\n",
        "print('The total count of different POS tags in the WSJ subset is {}.'.format(count))\n",
        "\n",
        "count = 0\n",
        "for x in swda_POSdist.keys():\n",
        "    count += 1\n",
        "print('The total count of different POS tags in the SWDA corpus is {}.'.format(count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEf-oXVQKqwo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sort in descending order before plotting\n",
        "sorted_WSJ = sorted(WSJ_POSdist.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_SWDA = sorted(swda_POSdist.items(), key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CArH08ldLvaS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# rotate the x tick markers to try to help reduce clutter\n",
        "plt.xticks(rotation=50)\n",
        "# unpacks and plots the list of tuples\n",
        "plt.bar(*zip(*sorted_SWDA), color='#FFD700')\n",
        "plt.ylabel('Relative frequency')\n",
        "plt.title('Relative frequency distribution of POS tags in SWDA corpus')\n",
        "\n",
        "plt.savefig('../swda_POSdist.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Ah1CGwpMLvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# rotate the x tick markers to try to help reduce clutter\n",
        "plt.xticks(rotation=50)\n",
        "plt.bar(*zip(*sorted_WSJ), color='#7F00FF')\n",
        "plt.ylabel('Relative frequency')\n",
        "plt.title('Relative frequency distribution of POS tags in WSJ subset')\n",
        "\n",
        "plt.savefig('../wsj_POSdist.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cL6F9WGPMXE5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorted_SWDA[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z2OansRSMbDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorted_WSJ[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNNWGk8iMipp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Personal pronouns (PRP) make a top ten appearance in the SWDA dataset, but not in the WSJ subset. This makes sense given the conversational nature of the SWDA dataset. Verbs, adverbs, and interjections, in the form of VBP, RB, and UH respectively also make an appearance in the SWDA dataset and not in the WSJ subset. Again, this is most likely due to the conversational nature of the SWDA corpus.\n",
        "\n",
        "On the flipside, nouns and adjectives, in the form of NN, NNP, NNS, JJ, make an appearance in the top ten for the WSJ subset. While only nouns in the form of NN make an appearance in the SWDA corpus. This, as with the verbs and adjectives previously discussed, is most like due to the differences in the nature of the two datasets. The WSJ subset contains descriptions of current events, which would lead to a greater number of nouns and adjectives.\n",
        "\n",
        "Now let's see if our suspicions about these dataset hold true. We will now statistically test whether or not these distrobutions are similiar using the kolmogorov-smirnov test. "
      ]
    },
    {
      "metadata": {
        "id": "cfzpQE24MdXi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def treebank_tagList():\n",
        "    \"\"\"Build a list of POS tags for the NLTK subset of the WSJ Treebank.\"\"\"\n",
        "    l = []\n",
        "    for fileid in treebank.fileids():\n",
        "        for word in treebank.tagged_words(fileid):\n",
        "            l.append(word[1])\n",
        "    return l\n",
        "\n",
        "def swda_tagList():\n",
        "    \"\"\"Gather and print relative frequency distribution of the tags.\"\"\"\n",
        "    l = []\n",
        "    corpus = CorpusReader('swda')\n",
        "    # Loop, counting tags:\n",
        "    for utt in tqdm(corpus.iter_utterances(display_progress=False)):\n",
        "        for x in np.asarray(utt.pos_lemmas(wn_lemmatize=False)):\n",
        "            l.append(x[1])\n",
        "    return l\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Shm2I3jIMq74",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wsj_tagList = treebank_tagList()\n",
        "swda_tagList = swda_tagList()\n",
        "print('The length of the SWDA tag list is {}. The length of the WSJ tag list is {}.'.format(len(swda_tagList), len(wsj_tagList)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLG3B0-6PAKd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run a kolmogorov-smirnov two sample test to see if these dataset came from the same distribution\n",
        "# use this to estimate the similarity of the two distributions\n",
        "# null hypothesis ( > p = 0.05) is that they do indeed come from the same distribution\n",
        "ks_2samp(wsj_tagList, swda_tagList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A7nNG8hEPKmx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The result of the ks test is to reject our null hypotheses. That is, these two distributions do not come from the same distribution. This is expected given that I left in all of the extra POS tags that appear in the SWDA and not in the WSJ subset. In addition, we observed the substantial difference in length, which can influence the result of the KS test. Let's see what the result is if I narrow down the distributions to only contain POS tags that appear in both sets."
      ]
    },
    {
      "metadata": {
        "id": "oZT4Tr0EPJA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generate new lists that contain only POS tags that occur in both sets\n",
        "wsj_df = pd.DataFrame(wsj_tagList)\n",
        "wsj_df.columns = ['POS']\n",
        "swda_df = pd.DataFrame(swda_tagList)\n",
        "swda_df.columns = ['POS']\n",
        "\n",
        "swda_sharedList = swda_df[swda_df.POS.isin(wsj_df['POS'])]['POS'].tolist()\n",
        "wsj_sharedList = wsj_df[wsj_df.POS.isin(swda_df['POS'])]['POS'].tolist()\n",
        "\n",
        "print('The shared SWDA list contains {} values after dropping {} unshared values.'.format(len(swda_sharedList),\n",
        "                                                                                        len(swda_df)-len(swda_sharedList)))\n",
        "print('The shared WSJ list contains {} values after dropping {} unshared values.'.format(len(wsj_sharedList),\n",
        "                                                                                        len(wsj_df)-len(wsj_sharedList)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ztxFnavoTk7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# find the shared POS between the two corpuseses and their associated probability of appearance\n",
        "shared_POS_WSJ = []\n",
        "for key in swda_POSdist:\n",
        "    if key in WSJ_POSdist:\n",
        "        shared_POS_WSJ.append((key, WSJ_POSdist[key]))\n",
        "\n",
        "shared_POS_SWDA = []\n",
        "for key in WSJ_POSdist:\n",
        "    if key in swda_POSdist:\n",
        "        shared_POS_SWDA.append((key, swda_POSdist[key]))\n",
        "print('There are {} POS tags that shared between the two datasets.'.format(len(shared_POS_WSJ)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bb8UMbbTpqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# converts and combines the shared POS tags and their respective relative frequencies distributions\n",
        "# into a neat dataframe\n",
        "df_wsj = pd.DataFrame(shared_POS_WSJ)\n",
        "df_wsj.columns = ['POS', 'WSJ']\n",
        "df_swda = pd.DataFrame(shared_POS_SWDA)\n",
        "df_swda.columns = ['POS', 'SWDA']\n",
        "df = pd.merge(df_wsj, df_swda, on='POS')\n",
        "# print first 10 rows, can remove [:10] to view all\n",
        "df[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oOIrDu2ETyws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# look at the shared tag occurrences as group barplot\n",
        "df.index = df['POS']\n",
        "df.plot(kind='bar', title='Shared POS tag distribution')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuG8mrc1UYaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Setting the width for the bars\n",
        "width = 0.25 \n",
        "\n",
        "# Bar plots\n",
        "fig, ax = plt.subplots(figsize=(20,5))\n",
        "plt.xticks(rotation=50)\n",
        "plt.bar(df['POS'], \n",
        "        df['WSJ'], \n",
        "        width, \n",
        "        alpha=0.7, \n",
        "        color='#7F00FF', \n",
        "        label='WSJ')\n",
        "\n",
        "plt.bar(df['POS'], \n",
        "        df['SWDA'], \n",
        "        width, \n",
        "        alpha=0.7, \n",
        "        color='#FFD700',\n",
        "        label='SWDA')\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Relative frequency')\n",
        "plt.title('Relative frequency distribution of shared POS tags')\n",
        "plt.savefig('../shared_dist.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLQdtnPrQxcJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now lets pit these datasets up against each other as before maintaining the same null hypothesis,\n",
        "# that these datasets come from the same distrobution\n",
        "ks_2samp(wsj_sharedList, swda_sharedList)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rIEJCcpFQ7xd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# now lets try to equate for length by randomly sampling the same number of items from the SWDA list.\n",
        "swda_tagCompare = np.random.choice(swda_sharedList, size=len(wsj_sharedList), replace=False)\n",
        "len(swda_tagCompare)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "njUxWS_HTJFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ks_2samp(wsj_sharedList, swda_tagCompare)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KglhB8fAV8we",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So these two datasets have very different distributions of parts of speech. How do you think a language model trained on one set would perform on the other? Would it generalize well? Why or why not?\n",
        "\n",
        "Now it's time to focus in on the dataset that we will be using in our language model."
      ]
    },
    {
      "metadata": {
        "id": "orkJ8X5qTd4s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# here we pull the text from all the utterances that contain trees\n",
        "d = defaultdict(int)\n",
        "corpus = CorpusReader('swda')\n",
        "for utt in tqdm(CorpusReader('swda').iter_utterances(display_progress=False)): \n",
        "    if utt.trees:\n",
        "        d[utt.text] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Efee7-3bVrJ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# then we sort by number of observences and show the top 50\n",
        "sorted_SWDAtext = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_SWDAtext[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uvv0RSIFGSb-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lots of different versions of the same words, this will get handled when we work on the model by lemmatizing the input."
      ]
    },
    {
      "metadata": {
        "id": "TqEa1O6gYnf0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for completeness lets check the least frequent\n",
        "sorted_SWDAtext[-10:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MnfJ7jNuYyZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stick it into a nice dataframe before plotting\n",
        "df_text = pd.DataFrame(sorted_SWDAtext)\n",
        "df_text.columns = ['Phrase', 'count']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RywHFtnfY3w8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Setting the positions and width for the bars\n",
        "width = 0.25 \n",
        "    \n",
        "# Plotting the bars\n",
        "fig, ax = plt.subplots(figsize=(20,5))\n",
        "\n",
        "plt.xticks(rotation=70)\n",
        "plt.bar(df_text['Phrase'][:50], \n",
        "        df_text['count'][:50],\n",
        "        width, \n",
        "        alpha=0.7, \n",
        "        color='#FFD700')\n",
        "plt.ylabel('Number of occurrences')\n",
        "plt.xlabel('Phrase')\n",
        "plt.title('Top 50 phrases in SWDA corpus')\n",
        "plt.savefig('../phrase_dist.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BpYPbYv2Y-s2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# let's double check the distributions of utterances including those without trees\n",
        "# here we pull the text from all the utterances that contain trees\n",
        "d_full = defaultdict(int)\n",
        "for utt in tqdm(CorpusReader('swda').iter_utterances(display_progress=False)): \n",
        "    d_full[utt.text] += 1\n",
        "\n",
        "[(\n",
        "sorted_SWDAtextFull = sorted(d_full.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_SWDAtextFull[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pLLvOCLJaRhv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "shared_SWDAphrase = []\n",
        "for key in d:\n",
        "    if key in d_full:\n",
        "        shared_SWDAphrase.append((key, d_full[key]))\n",
        "\n",
        "df_sharedPhrase = pd.DataFrame(shared_SWDAphrase)\n",
        "df_sharedPhrase.columns = ['Phrase', 'count']\n",
        "df_sharedPhrase.sort_values(['count'], ascending=False)[:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I4Vkxq3aX6R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Setting the positions and width for the bars\n",
        "width = 0.25 \n",
        "    \n",
        "# Plotting the bars\n",
        "fig, ax = plt.subplots(figsize=(20,5))\n",
        "\n",
        "plt.xticks(rotation=70)\n",
        "plt.bar(df_text['Phrase'][:50], \n",
        "        df_text['count'][:50],\n",
        "        width, \n",
        "        alpha=0.7, \n",
        "        color='#7F00FF',\n",
        "        label='has tree')\n",
        "\n",
        "plt.bar(df_sharedPhrase.sort_values(['count'], ascending=False)['Phrase'][:50], \n",
        "        df_sharedPhrase.sort_values(['count'], ascending=False)['count'][:50],\n",
        "        width, \n",
        "        alpha=0.7, \n",
        "        color='#FFD700',\n",
        "        label='tree + no tree')\n",
        "\n",
        "plt.ylabel('Number of occurrences')\n",
        "plt.xlabel('Phrase')\n",
        "plt.title('Top 50 phrases in SWDA corpus with and without a tree')\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGb6BLl9cIhT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we are going to switch gears and start some modeling. For this modeling problem we are going to predict the dialog speech act based on the utterance. \n",
        "\n",
        "Typically you would seperate the data exploration and modeling into two different scripts, but I kept them together for the ease of presentation."
      ]
    },
    {
      "metadata": {
        "id": "ofaPK3M5bFWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data preprocessings\n",
        "import re\n",
        "from collections import OrderedDict\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# model functions\n",
        "from tensorflow import set_random_seed\n",
        "from tensorflow.contrib.keras.api.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.contrib.keras.api.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.contrib.keras.api.keras.models import Sequential\n",
        "from tensorflow.contrib.keras.api.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, \\\n",
        "    Embedding, Bidirectional\n",
        "\n",
        "# tensorboard and tSNE\n",
        "from time import time\n",
        "from tensorflow.contrib.keras.api.keras.callbacks import TensorBoard\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gStD0quFc51n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# start by setting a random seed for reproducible results\n",
        "#np.random.seed(1)\n",
        "#set_random_seed(1)\n",
        "\n",
        "# asks for current run number in order to organize logs and weight saves\n",
        "run_number = int(input('What run number is this? '))\n",
        "# calls the user to input a number of epoch to run through\n",
        "nbEpochs = int(input('How many epochs of training would you like to run? '))\n",
        "\n",
        "# In order to build a model to predict the damsl act tags associated with each utterance\n",
        "# I first need to clean up the text a little bit. I am going to follow the guidelines used\n",
        "# by Zhao & Kawahara (2018) and remove the , . / ! ? marks that appear in each utterance.\n",
        "# It may be beneficial to keep them, maybe something to look into at a future date.\n",
        "\n",
        "# Zhao, T., & Kawahara, T. (2018). A Unified Neural Architecture for Joint Dialog\n",
        "# Act Segmentation and Recognition in Spoken Dialog System.\n",
        "# In Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue (pp. 201-208).\n",
        "#\n",
        "# I am going to reconstruct each utterance from the pos_lemmas structure as they are already\n",
        "# relatively clean.\n",
        "excl = [',', '.', '!', '?', '/', '--']\n",
        "\n",
        "print(\"Proceeding to gather data from the SWDA corpus...\")\n",
        "\n",
        "def phrase_act():\n",
        "    \"\"\"Gathers usable phrases from the swda corpus.\n",
        "    Produces a dictionary of phrase:damsl_act pairs.\"\"\"\n",
        "    d = OrderedDict()\n",
        "    corpus = CorpusReader('swda')\n",
        "    for utt in tqdm(corpus.iter_utterances(display_progress=False), ascii=True):\n",
        "        utterance = []\n",
        "        for x in np.asarray(utt.pos_lemmas(wn_lemmatize=False)):\n",
        "            if x[0] not in excl:\n",
        "                utterance.append(x[0])\n",
        "            phrase = \" \".join(utterance)\n",
        "            # remove space before ' and n'\n",
        "            phrase_clean = re.sub(r\"[\\s+](?='|n')\", '', phrase)\n",
        "            # splits the phrase into single words\n",
        "            phrase_split = phrase_clean.split()\n",
        "            # stems the words as english and lowercases everything\n",
        "            # the idea and code for stemming came from:\n",
        "            # https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n",
        "            stemmer = SnowballStemmer('english')\n",
        "            stemmed_words = [stemmer.stem(word) for word in phrase_split]\n",
        "            text = \" \".join(stemmed_words)\n",
        "        # I picked the damsl act tag as my speech action label because it reduces the number of action tags in the dataset\n",
        "        # from over 200, to 43. it says 44 in http://compprag.christopherpotts.net/swda.html,\n",
        "        # but my label count says 43.\n",
        "        d[text] = utt.damsl_act_tag()\n",
        "    return d\n",
        "\n",
        "phrase_dict = phrase_act()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6X9zTAAihNuU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# change to list because pandas don't like dictionaries (they can't read)\n",
        "phrase_list = []\n",
        "for x,v in phrase_dict.items():\n",
        "    phrase_list.append((x, v))\n",
        "print(\"The total dataset is made up of {} samples.\".format(len(phrase_list)))\n",
        "\n",
        "# Here I will be adapting a medium post:\n",
        "# https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n",
        "# focused on yelp reviews, to the classification problem at hand.\n",
        "df = pd.DataFrame(phrase_list)\n",
        "df.columns = ['Phrase', 'Act']\n",
        "# remove any empty rows\n",
        "df = df.dropna()\n",
        "# one hot encode the damsl act tags\n",
        "labels = pd.get_dummies(df['Act'])\n",
        "labels = labels.values\n",
        "print(\"There are {} possible acts in this dataset.\".format(len(labels[0])))\n",
        "print(\"Tokenizing and generating sequences...\")\n",
        "\n",
        "# Create sequence, limiting vocab size to 15000\n",
        "vocabulary_size = 15000\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(df['Phrase'])\n",
        "\n",
        "# and cutting phrase length off at 20 words. This will pad sequences below 20 up to 20, putting the\n",
        "# meaningful text at the end of the sequence.\n",
        "sequences = tokenizer.texts_to_sequences(df['Phrase'])\n",
        "data = pad_sequences(sequences, maxlen=20)\n",
        "print(\"Done.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A98L08R-LJ3v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer.word_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJscLgT1NOMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6SrNtD-mL-Xz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for x in tokenizer.word_index.keys():\n",
        "    count += 1\n",
        "print('The total count of different words in the tokenizer is {}.'.format(count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BE3vnvZRN4EL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['Phrase'][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4GMI3WVL6aCm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['Act'][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-2Qy-pmK9Ed",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2rozAZVjpva",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a model that predicts the speech act of the provided utterance...\n",
        "# https://keras.io/layers/ contains all the documentation for every layer\n",
        "\n",
        "# initiate the model\n",
        "model = Sequential()\n",
        "# Embedding layer is first in this sort of task, with vocab size, embedding size, and sequence length as parameters\n",
        "# embeds in a hundred dimensions, can change if you want a more sparse embedding.\n",
        "model.add(Embedding(vocabulary_size, 100, input_length=20, name=\"Embed\"))\n",
        "# adding the 1D convolutional layer will speed up training\n",
        "model.add(Conv1D(64, 5, activation='relu', name=\"Conv1\"))\n",
        "# grab the max activation within pool size, stride size defaults to pool size which is 4 in this case\n",
        "model.add(MaxPooling1D(pool_size=4, name=\"Max1\"))\n",
        "# single LSTM layer with 20% dropout, default activation is tanh, default recurrent activation is hard sigmoid\n",
        "# this layer initializes with a glorot uniform distribution in the layer and an orthogonal initializer in the recurrence\n",
        "model.add(Bidirectional(LSTM(100, activation='tanh', dropout=0.2, recurrent_dropout=0.2, name=\"BI-LSTM1\")))\n",
        "# use softmax activation function on the final dense (fully connected) layer\n",
        "# because it works well with multiclass classification\n",
        "model.add(Dense(len(labels[0]), activation='softmax', name=\"FC1\"))\n",
        "\n",
        "# compile the network with categorical cross entropy loss function\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BnNGKJgUkCUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save logs for use with tensorboard\n",
        "tensorboard = TensorBoard(log_dir='../logs/run{}/{}'.format(run_number, time()), histogram_freq=0,\n",
        "                          write_graph=True, write_grads=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DbxYhquTkHIs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train the model on an 80/20 split for a user decided number of epochs\n",
        "# defaults to a 32 batch size\n",
        "model.fit(data, labels,\n",
        "          batch_size=16,\n",
        "          validation_split=0.2, epochs=nbEpochs,\n",
        "          verbose=1, callbacks=[tensorboard])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XVKJMF25S9oY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# When you have a categorical problem that contains a large number of categories\n",
        "# and your accuracy shoots up to 60-70% after just a couple batches, you should be\n",
        "# aware that something is amiss...the first thing you should do is check the \n",
        "# distribution of categories...\n",
        "df['Act'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_JQfhJ0kQ9d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save(\"../model_run{}_epochs{}.h5\".format(run_number, nbEpochs))\n",
        "print(\"Saved model to parent folder.\")\n",
        "\n",
        "print('To view tensorboard, open up either Firefox or Chrome and type localhost:6006 in the address bar.')\n",
        "print('Then run `tensorboard --logdir=logs/run{}/` in your terminal.'.format(run_number))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hY06bPwypUJD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# finally lets take a look at the word embeddings using tSNE\n",
        "\n",
        "# first we need to grab the weights from the embedding layer\n",
        "word_embds = model.layers[0].get_weights()[0]\n",
        "\n",
        "print('Generating a visualization of the word embeddings of your model...')\n",
        "# then use tSNE to reduce dimensionality from 100 to 2\n",
        "# the shape of this structure will change as the number of epochs increase\n",
        "tsne_embds = TSNE(n_components=2).fit_transform(word_embds)\n",
        "\n",
        "# then plot\n",
        "plt.scatter(tsne_embds[:,0], tsne_embds[:,1])\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('tSNE reduction of word embeddings to 2 dimensions')\n",
        "\n",
        "plt.savefig('../tSNE_wordembeds.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
